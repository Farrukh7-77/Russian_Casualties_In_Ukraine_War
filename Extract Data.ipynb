{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd603556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re \n",
    "import time \n",
    "\n",
    "\n",
    "BASE_URL = \"https://index.minfin.com.ua/en/russian-invading/casualties/\"\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "\n",
    "\n",
    "\n",
    "def get_html_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=15, headers=HEADERS)\n",
    "        response.raise_for_status() \n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return None\n",
    "\n",
    "def find_all_month_links_updated(soup_obj, base_url):\n",
    "    month_links = {}\n",
    "    ajax_months = soup_obj.find_all('div', class_='ajaxmonth')\n",
    "    \n",
    "    for month_div in ajax_months:\n",
    "        h4_element = month_div.find('h4', class_='normal')\n",
    "        if h4_element:\n",
    "            link_element = h4_element.find('a')\n",
    "            if link_element:\n",
    "                month_name = link_element.text.strip()\n",
    "                relative_link = link_element.get('href')\n",
    "                \n",
    "                domain = \"/\".join(base_url.split('/')[:3]) + \"/\" \n",
    "                full_url = domain + relative_link \n",
    "                \n",
    "                month_code = relative_link.split('/')[-2]\n",
    "                month_links[month_name] = {'url': full_url, 'code': month_code}\n",
    "    return month_links\n",
    "\n",
    "def extract_all_daily_data(html_content, month_code=None):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    final_long_data = []\n",
    "    \n",
    "    daily_sections = soup.find_all('li', class_='gold')\n",
    "    \n",
    "    for section in daily_sections:\n",
    "        current_date_text = None\n",
    "        date_span = section.find('span', class_='black')\n",
    "        \n",
    "        if date_span:\n",
    "            current_date_text = date_span.text.strip()\n",
    "        \n",
    "        if not current_date_text:\n",
    "            continue\n",
    "\n",
    "        casualties_div = section.find('div', class_='casualties')\n",
    "        \n",
    "        if casualties_div:\n",
    "            casualty_items = casualties_div.find_all('li')\n",
    "            \n",
    "            for item in casualty_items:\n",
    "                data_text = item.text.strip()\n",
    "                \n",
    "                if '—' in data_text:\n",
    "                    key_part, value_part = data_text.split('—', 1) \n",
    "                    \n",
    "                    clean_key = key_part.strip().replace('\\xa0', ' ')\n",
    "                    if item.find('abbr'):\n",
    "                        clean_key = item.find('abbr').text.strip()\n",
    "                    elif 'missiles' in clean_key:\n",
    "                        clean_key = 'Cruise missiles'\n",
    "                    \n",
    "                    total_value_raw = value_part.split('(')[0].strip()\n",
    "                    \n",
    "                    total_value_raw = re.sub(r'[^\\d]', '', total_value_raw) \n",
    "                    \n",
    "                    final_value = None\n",
    "                    if total_value_raw:\n",
    "                        try:\n",
    "                            final_value = int(total_value_raw)\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                            \n",
    "                    if final_value is not None:\n",
    "                        final_long_data.append({\n",
    "                            'Date': current_date_text,\n",
    "                            'Equipment type': clean_key,\n",
    "                            'Total': final_value\n",
    "                        })\n",
    "                        \n",
    "    return final_long_data\n",
    "\n",
    "def fetch_full_month_data(month_code, initial_html, month_name):\n",
    "    \n",
    "    all_month_data = []\n",
    "    \n",
    "    if initial_html:\n",
    "        month_data = extract_all_daily_data(initial_html, month_code) \n",
    "        all_month_data.extend(month_data)\n",
    "    \n",
    "    offset = len(month_data) \n",
    "    \n",
    "    if offset > 0:\n",
    "        print(f\"   [INFO - {month_name}] Initial fetch of {offset} days. Looking for more data...\")\n",
    "    \n",
    "    max_ajax_requests = 20\n",
    "    request_count = 0\n",
    "    \n",
    "    while request_count < max_ajax_requests:\n",
    "        request_count += 1\n",
    "        \n",
    "        ajax_url = f\"https://index.minfin.com.ua/ajax/casualties/day_by_day.php?month={month_code}&offset={offset}&offset_month={month_code}\"\n",
    "        \n",
    "        ajax_html = get_html_content(ajax_url)\n",
    "        \n",
    "        if not ajax_html or 'li class=\"gold\"' not in ajax_html:\n",
    "            break\n",
    "        \n",
    "        additional_data = extract_all_daily_data(ajax_html, month_code)\n",
    "        \n",
    "        if not additional_data:\n",
    "            break\n",
    "            \n",
    "        all_month_data.extend(additional_data)\n",
    "        \n",
    "        offset += len(additional_data)\n",
    "        \n",
    "        print(f\"   [INFO - {month_name}] Fetched {len(additional_data)} new days. Total days: {offset}\")\n",
    "        \n",
    "        if len(additional_data) < 10: \n",
    "            break\n",
    "            \n",
    "        time.sleep(0.5) \n",
    "\n",
    "    return all_month_data\n",
    "\n",
    "def master_scraper_to_excel():\n",
    "    \n",
    "    print(f\"1. Fetching base URL: {BASE_URL}\")\n",
    "    initial_html = get_html_content(BASE_URL)\n",
    "    \n",
    "    if not initial_html:\n",
    "        print(\"Initial HTML could not be fetched. Program stopped.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(initial_html, 'html.parser')\n",
    "    all_casualties_data = []\n",
    "\n",
    "    try:\n",
    "        current_month_title = soup.find('div', class_='title').find('h1').text.strip().split('for')[1].split('by')[0].strip()\n",
    "    except:\n",
    "        current_month_title = \"Current Month\"\n",
    "    current_month_code = \"2025-11\" \n",
    "        \n",
    "    print(f\"\\n2.  **{current_month_title}** month data (with full AJAX) is being fetched...\")\n",
    "    \n",
    "    current_month_data = fetch_full_month_data(current_month_code, initial_html, current_month_title)\n",
    "    all_casualties_data.extend(current_month_data)\n",
    "    print(f\"Final {len(current_month_data)} daily records added for **{current_month_title}**.\")\n",
    "    \n",
    "    print(\"\\n3. Finding links and codes for all old months...\")\n",
    "    all_month_links = find_all_month_links_updated(soup, BASE_URL)\n",
    "    \n",
    "    for month_name, link_info in all_month_links.items():\n",
    "        url = link_info['url']\n",
    "        month_code = link_info['code']\n",
    "        \n",
    "        print(f\"\\n4.  **{month_name}** month data (with full AJAX) is being fetched...\")\n",
    "        \n",
    "        month_initial_html = get_html_content(url) \n",
    "        \n",
    "        if month_initial_html:\n",
    "            month_data = fetch_full_month_data(month_code, month_initial_html, month_name)\n",
    "            all_casualties_data.extend(month_data)\n",
    "            print(f\"Final {len(month_data)} daily records added for **{month_name}**.\")\n",
    "            \n",
    "    df = pd.DataFrame(all_casualties_data)\n",
    "\n",
    "    excel_file_name = 'russia_military_casualties.xlsx'\n",
    "    \n",
    "    if not df.empty:\n",
    "        df = df[['Date', 'Equipment type', 'Total']].drop_duplicates() \n",
    "        df.to_excel(excel_file_name, index=False)\n",
    "        \n",
    "        print(\"\\n\" + (\"=\"*70))\n",
    "        print(f\" **Project Successfully Completed!**\")\n",
    "        print(f\" A total of **{len(df)}** unique records were scraped and written to '{excel_file_name}'.\")\n",
    "        print(\" Your Excel file is in the same folder.\")\n",
    "        print(\"=\"*70)\n",
    "    else:\n",
    "        print(\"\\n Warning: No data was scraped. Excel file was not created.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    master_scraper_to_excel()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
